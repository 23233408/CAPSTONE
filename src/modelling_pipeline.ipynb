{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline to automate the model fitting, tuning and evaluation process.\n",
    "\n",
    "#todo:\n",
    "* KNN, decision tree, bagging, XGBoost\n",
    "* (AUROCs) for performance evaluation of multiple models\n",
    "* Calibration plots to visualise candidate model predictions\n",
    "* SHAP plot - For best-performing model, identify & report significance of parameters\n",
    "\n",
    "#todo: questions\n",
    "* should we set up n folds for CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np, warnings\n",
    "from pathlib import Path\n",
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "# visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data processing\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_validate, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# model comparison\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from xgboost import XGBClassifier\n",
    "# from sklearn.ensemble import AdaBoostClassifier\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import cohen_kappa_score,classification_report \n",
    "from sklearn.metrics import mean_squared_error, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score, auc, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, balanced_accuracy_score, PrecisionRecallDisplay\n",
    "import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Obtain input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the parameters\n",
    "ROOT_DIR = Path('')\n",
    "\n",
    "model_input_path = ROOT_DIR / 'data' / 'model_input' / 't0_v3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dfs(ROOT_DIR, tops, date):\n",
    "    dfs_dict = {}\n",
    "    for top in tops:\n",
    "        dfs_dict[f\"top{top}\"] = get_model_input_dfs(ROOT_DIR, f'top{top}', date)\n",
    "    return dfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_dfs(ROOT_DIR, tops, date):\n",
    "    dfs_dict = {}\n",
    "    for top in tops:\n",
    "        dfs_dict[f\"top{top}\"] = get_model_full_dfs(ROOT_DIR, f'top{top}', date)\n",
    "    return dfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_dfs(ROOT_DIR, top, version, timepoints=['t0', 't1', 't2', 't3', 't4']):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to read the model input dataframes based on provided prefixes and date string.\n",
    "    \n",
    "    Parameters:\n",
    "    - ROOT_DIR: The root directory path.\n",
    "    - prefix: The prefix (e.g. 'top20', 'top30', etc.)\n",
    "    - date_str: The date string in the filename (e.g. '20231005').\n",
    "    - timepoints: A list of timepoints (default is ['t0', 't1', 't2', 't3', 't4']).\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with keys as timepoints and values as the corresponding dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    dfs = {}\n",
    "    for tp in timepoints:\n",
    "        path = ROOT_DIR / 'data' / 'Model input data' / f\"{tp}_{top}_{version}.csv\"\n",
    "        dfs[tp] = get_model_input_df(path)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_full_dfs(ROOT_DIR, top, version, timepoints=['t0', 't1', 't2', 't3', 't4']):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to read the model input dataframes based on provided prefixes and date string.\n",
    "    \n",
    "    Parameters:\n",
    "    - ROOT_DIR: The root directory path.\n",
    "    - prefix: The prefix (e.g. 'top20', 'top30', etc.)\n",
    "    - date_str: The date string in the filename (e.g. '20231005').\n",
    "    - timepoints: A list of timepoints (default is ['t0', 't1', 't2', 't3', 't4']).\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary with keys as timepoints and values as the corresponding dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    dfs = {}\n",
    "    for tp in timepoints:\n",
    "        path = ROOT_DIR / 'data' / 'Model input data' / f\"{tp}_{top}_{version}.csv\"\n",
    "        dfs[tp] = get_model_input_df(path)\n",
    "    \n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_input_df(model_input_path):\n",
    "  \"\"\"_summary_\n",
    "  \n",
    "  Args:\n",
    "      model_input_path (_type_): Processed csv file path.\n",
    "  \"\"\"\n",
    "  model_input_df = pd.read_csv(model_input_path)\n",
    "  model_input_df = model_input_df.drop(columns=[\"SUBJECT_ID\", \"HADM_ID\"])\n",
    "  return model_input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_values(time_df):\n",
    "    for time_point, df in time_df.items():\n",
    "        df_without_target = df.drop(columns=['IS_SEPSIS'])\n",
    "        total_values = df_without_target.size\n",
    "        count_999 = (df_without_target == -999).sum().sum()\n",
    "        missing_proportions = count_999 / total_values *100\n",
    "        print(f\"Number of missing values in {time_point}: {count_999} ({missing_proportions:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_SOFA(time_df):\n",
    "    for time_point, df in time_df.items():\n",
    "        total_SOFA = df.shape[0]\n",
    "        count_SOFA_999 = (df['SOFA'] == -999).sum().sum()\n",
    "        missing_SOFA_proportions = count_SOFA_999 / total_SOFA *100\n",
    "        print(f\"Number of missing SOFA in {time_point}: {count_SOFA_999} ({missing_SOFA_proportions:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Train/test split & Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df_train):\n",
    "    \"\"\"\n",
    "    Split data into training and test sets.\n",
    "    Standardise numerical features.\n",
    "    \n",
    "    Compare performances of different models.\n",
    "    Perform hyperparameter tuning on best model.\n",
    "    Validate the optimal model's performance on the test set.\n",
    "    Generate predictions for unknown data using the optimal model.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_train: Processed data for model training\n",
    "    - test_size: \n",
    "    \n",
    "    Returns:\n",
    "    - X train, X test, y train, y test \n",
    "    \"\"\"\n",
    "         \n",
    "    # Target and Predictors\n",
    "    X = df_train.drop('IS_SEPSIS', axis='columns')\n",
    "    y = df_train['IS_SEPSIS']\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "    \n",
    "    # Fit the scaler and transform the X train and test sets\n",
    "    # Standardising (not normalising!)\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_weights(y_train):\n",
    "  \"\"\"\n",
    "      Compute class weights for balancing the classes in the target variable.\n",
    "\n",
    "  Args:\n",
    "      y_train (_type_)\n",
    "\n",
    "  Returns:\n",
    "      class_weights (dict)\n",
    "  \"\"\"\n",
    "  # Get sepsis label proportions\n",
    "  label_counts = y_train.value_counts()\n",
    "  label_proportions = label_counts / len(y_train)*100\n",
    "    \n",
    "  # Compute class weights\n",
    "  class_weights = {0: 1 / (label_proportions[0] / 100), 1: 1 / (label_proportions[1] / 100)}\n",
    "\n",
    "  # Round the class weights to the desired precision (optional)\n",
    "  class_weights = {key: round(weight, 4) for key, weight in class_weights.items()}\n",
    "  \n",
    "  return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modeling Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Algorithms to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise candidate models\n",
    "candidate_models = {\n",
    "    'Logistic_Regression': LogisticRegression(max_iter=10000000000, class_weight=class_weights, C=0.1, penalty=None),\n",
    "    'Random_Forest': RandomForestClassifier(class_weight=class_weights, max_depth=7, min_samples_leaf=25, min_samples_split=250),\n",
    "    'Gradient_Boosting': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=901)\n",
    "    \n",
    "    \n",
    "    # 'SGD_Classifier': SGDClassifier(class_weight=class_weights),\n",
    "    # 'XGB': XGBClassifier(),\n",
    "    # 'KNeighbors': KNeighborsClassifier(),\n",
    "    # 'Adaboost':AdaBoostClassifier()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model training & Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def static_models(candidate_models, class_weights, X_train, y_train, X_test, y_test, time):\n",
    "    \n",
    "    performance_df = pd.DataFrame(columns=['Model', 'Balanced_Acc_Train', 'Balanced_Acc_Test', 'Precision_Train', 'Precision_Test', 'Recall_Train', 'Recall_Test', 'F1_Train', 'F1_Test'])\n",
    "\n",
    "    for model_name, model in candidate_models.items():\n",
    "\n",
    "        y_pred_train, y_pred_test = fit_models(model, model_name, class_weights, X_train, y_train, X_test)\n",
    "\n",
    "        performance_scores = get_performance_scores(model, model_name, X_train, X_test, y_train, y_test, y_pred_train, y_pred_test)\n",
    "\n",
    "        formatted_model_name = f\"{model_name}_{time}\"\n",
    "        new_row = pd.DataFrame([[formatted_model_name] + performance_scores], columns=performance_df.columns)\n",
    "\n",
    "        performance_df = pd.concat([performance_df, new_row], ignore_index=True)\n",
    "\n",
    "        predicted_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "    return performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cv_analysis(X_train, y_train, candidate_models, class_weights):\n",
    "    \"\"\"\n",
    "    Perform cross-validation analysis on a set of candidate models and return their mean scores.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Scaled feature matrix for training.\n",
    "        y_train (pd.Series): Target labels for training.\n",
    "        candidate_models (Dict[str, Any]): A dictionary of static ML model names and their respective instantiated models.\n",
    "        class_weights (Dict[int, float]): A dictionary of class weights for handling sepsis class imbalance.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing model names and their average cross-validation scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_names = []\n",
    "    model_average_scores = [] \n",
    "   \n",
    "   # Calculate mean scores using cross validation\n",
    "    for model_name, model in candidate_models.items():\n",
    "        scores = cross_val_score(model, X_train, y_train)\n",
    "        model_names.append(model_name)\n",
    "        model_average_scores.append(scores.mean())\n",
    "        \n",
    "    # Store mean scores for each model\n",
    "    df_model = pd.DataFrame({\n",
    "        'model': model_names,\n",
    "        'average_score': model_average_scores\n",
    "    })\n",
    "    \n",
    "    \n",
    "    return(df_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1 Model training function (inside models_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_models(model, model_name, class_weights, X_train, y_train, X_test):\n",
    "  \"\"\"_summary_\n",
    "\n",
    "  Args:\n",
    "      model (_type_): _description_\n",
    "\n",
    "  Returns:\n",
    "      _type_: _description_\n",
    "  \"\"\"\n",
    "  \n",
    "  if model_name == 'Gradient_Boost':\n",
    "    model.fit(X_train, y_train, sample_weight=[class_weights[label] for label in y_train])\n",
    "  else:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "  y_pred_train = model.predict(X_train)\n",
    "  y_pred_test = model.predict(X_test)\n",
    "  \n",
    "  return y_pred_train, y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2 Model evaluation function (inside models_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_performance_scores(model, model_name, X_train, X_test, y_train, y_test, y_pred_train, y_pred_test):\n",
    "  \"\"\"\n",
    "  Get performance measures on (i) ballanced accuracy (ii) precision, (iii) recall and (iv) F1 score.\n",
    "  \n",
    "  Args:\n",
    "      model (_type_): _description_\n",
    "      X_train (_type_): Scaled X_train\n",
    "      X_test (_type_): Scaled X_train\n",
    "      y_train (_type_): _description_\n",
    "      y_test (_type_): _description_\n",
    "  \n",
    "  Returns:\n",
    "  \"\"\"\n",
    "  # y_pred_train = model.predict(X_train)\n",
    "  # y_pred_test = model.predict(X_test)\n",
    "\n",
    "  # Computing balanced accuracy\n",
    "  balanced_acc_train = balanced_accuracy_score(y_train, y_pred_train)\n",
    "  balanced_acc_test = balanced_accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "  # Computing precision and recall\n",
    "  precision_train = precision_score(y_train, y_pred_train)\n",
    "  precision_test = precision_score(y_test, y_pred_test)\n",
    "  recall_train = recall_score(y_train, y_pred_train)\n",
    "  recall_test = recall_score(y_test, y_pred_test)\n",
    "\n",
    "  # Computing F1 score\n",
    "  f1_train = f1_score(y_train, y_pred_train)\n",
    "  f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "  # Format scores\n",
    "  performance_scores = [balanced_acc_train, balanced_acc_test, precision_train, precision_test, recall_train, recall_test, f1_train, f1_test]\n",
    "  \n",
    "  \n",
    "  # formatted_performance_scores = []\n",
    "  # for i in range(len(performance_scores)):\n",
    "  #   s = '({i:.4f})'.format(performance_scores[i])\n",
    "  #   formatted_performance_scores.append(s)\n",
    "  \n",
    "  # return formatted_performance_scores\n",
    "  return performance_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Evaluation (Visualisation - Plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(candidate_models, class_weights, X_train, X_test, y_train, y_test):\n",
    "\n",
    "  for model_name, model in candidate_models.items():\n",
    "\n",
    "    y_pred_train, y_pred_test = fit_models(model, model_name, class_weights, X_train, y_train, X_test)\n",
    "    predicted_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "    plot_confusion_matrix(model, model_name, X_train, X_test, y_train, y_test, y_pred_train, y_pred_test)\n",
    "    plot_precision_recall(model, model_name, X_test, y_test)\n",
    "    plot_roc_curve(predicted_probabilities, model_name, y_test, label = None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_roc_curves(candidate_models, X_test, y_test, title='ROC Curves'):\n",
    "    \"\"\"\n",
    "    This function plots the ROC curve for the provided models.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialise the plot\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    \n",
    "    # Iterate over classifiers and plot ROC curve for each\n",
    "    for model_name, model in candidate_models.items():\n",
    "        # Predict the probabilities of the positive class\n",
    "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test)\n",
    "        \n",
    "        # Compute ROC curve and AUC\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    # Plot the random classifier\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random Classifier (AUC = 0.50)')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Inside plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, model_name, X_train, X_test, y_train, y_test, y_pred_train, y_pred_test):\n",
    "  \n",
    "  # Computating the confusion matrix\n",
    "  cm_train = confusion_matrix(y_train, y_pred_train)\n",
    "  cm_test = confusion_matrix(y_test, y_pred_test)    \n",
    "\n",
    "  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "  \n",
    "  x_labels = [\"Predicted\\nNon-Sepsis\", \"Predicted\\nSepsis\"]\n",
    "  y_labels = [\"Actual Non-Sepsis\", \"Actual Sepsis\"]\n",
    "  sns.heatmap(cm_train, annot=True, fmt='d', xticklabels=x_labels, yticklabels=y_labels, ax=axes[0])\n",
    "  sns.heatmap(cm_test, annot=True, fmt='d', xticklabels=x_labels, yticklabels=y_labels, ax=axes[1])\n",
    "  \n",
    "  axes[0].set_title(\"CM in training set\", fontsize = 10)\n",
    "  axes[1].set_title(\"CM in test set\", fontsize = 10)\n",
    "  axes[0].tick_params(labelsize=9)\n",
    "  axes[1].tick_params(labelsize=9)\n",
    "  plt.tight_layout()\n",
    "  \n",
    "  print(model_name)\n",
    "  print(\" ---------------------------------------- \")\n",
    "  plt.show()\n",
    "\n",
    "#  plt.close(fig)\n",
    "\n",
    "#  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall(model, model_name, X_test, y_test):\n",
    "  # Plot precision recall\n",
    "  display = PrecisionRecallDisplay.from_estimator(\n",
    "    model, X_test, y_test, name=model_name, plot_chance_level=True\n",
    "  )\n",
    "  display.ax_.legend(loc='upper right')\n",
    "  _ = display.ax_.set_title(\"2-class Precision-Recall curve\")\n",
    "\n",
    "  fig = display.figure_\n",
    "  plt.show()\n",
    "\n",
    "#  return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(predicted_probabilities, model_name, y_test, label = None):\n",
    "  \"\"\"_summary_\n",
    "  returns AUC score, ROC curve, precision recall\n",
    "\n",
    "  Args:\n",
    "      fpr_test (array): _description_\n",
    "      tpr_test (array): _description_\n",
    "      model_name (str): Model name\n",
    "      label (_type_, optional): _description_. Defaults to None.\n",
    "  \"\"\"\n",
    "  # Plotting ROC curve\n",
    "  fpr, tpr, thresholds_roc_test = roc_curve(y_test, predicted_probabilities[:, 1], pos_label=1)\n",
    "\n",
    "  # Compute AUC score\n",
    "  auc_test = auc(fpr, tpr)\n",
    "  print(f\"{model_name} AUC : {auc_test:.4f}\")\n",
    "  \n",
    "  # Plot ROC curve\n",
    "  plt.plot(fpr, tpr, linewidth=2, label = label, color='orange')\n",
    "  plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal\n",
    "  plt.axis([0, 1, 0, 1])                                    \n",
    "  plt.xlabel('False Positive Rate (Fall-Out)', fontsize=11) \n",
    "  plt.ylabel('True Positive Rate (Recall)', fontsize=11)\n",
    "  plt.grid(False)  \n",
    "  \n",
    "  plt.title(\"ROC Curve: \" + model_name)\n",
    "  plt.figure(figsize=(6, 6))\n",
    "  plt.show()\n",
    "\n",
    "#  fig = plt.gcf()\n",
    "#  plt.close(fig)\n",
    "\n",
    "#  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tuning Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X_train, y_train, class_weights, candidate_models):\n",
    "    \"\"\"    \n",
    "    Tune hyperparameters for multiple classifiers using GridSearchCV.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame or np.array): Feature matrix for training.\n",
    "        y_train (pd.Series or np.array): Target labels for training.\n",
    "        class_weights (dict): Class weights for handling class imbalance.\n",
    "        candidate_models:\n",
    "\n",
    "    Returns:\n",
    "        dict: Best parameters for each classifier.\n",
    "    \"\"\"  \n",
    "\n",
    "    # define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'Logistic_Regression': {'C': [0.1, 1, 10],\n",
    "                                'penalty': ['l2', None]\n",
    "                            },\n",
    "        'Random_Forest': {'n_estimators': [50, 100, 150], \n",
    "                      'max_depth': [None, 10, 20], \n",
    "                        'min_samples_split': [2, 25, 50, 100, 250],\n",
    "                      'min_samples_leaf': [2, 25, 50, 100, 250]\n",
    "                      },\n",
    "        'Gradient_Boosting': {'n_estimators': [50, 100, 150, 200], \n",
    "                          'learning_rate': [0.01, 0.1, 0.2, 0.5], \n",
    "                          'loss': ['log_loss', 'exponential'], \n",
    "                          'criterion': ['friedman_mse', 'squared_error'],\n",
    "                          'max_features': ['sqrt', 'log2']\n",
    "                          }\n",
    "    }\n",
    "    \n",
    "    # Create an empty dictionary to store the best parameters for each model\n",
    "    best_params = {}\n",
    "\n",
    "    # Loop through the candidate_models dictionary to perform GridSearchCV for each model\n",
    "    for model_name, model in candidate_models.items():\n",
    "        \n",
    "        #todo: gridSearchCV or RandomizedSearchCV using refit=True?\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid[model_name], scoring='balanced_accuracy', cv=3)\n",
    "        \n",
    "        if model_name == 'Gradient_Boost':\n",
    "            grid_search.fit(X_train, y_train, sample_weight=[class_weights[label] for label in y_train])\n",
    "        else:\n",
    "            grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        # Store the best parameters for the current model\n",
    "        best_params[model_name] = grid_search.best_params_\n",
    "        best_models[model_name] = grid_search.best_estimator_\n",
    "                \n",
    "        # Get best score\n",
    "        best_score = grid_search.best_score_\n",
    "        print(f\"Best score for {model_name}: {best_score}\")\n",
    "        print(grid_search.best_params_)\n",
    "        \n",
    "\n",
    "        # hypertuned_model = final_model.fit(X_train, y_train)\n",
    "        \n",
    "    return best_params\n",
    "        #return(hypertuned_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Get model performances\n",
    "    model_performance_df = static_models(candidate_models, class_weights, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    # sort by model performance \n",
    "    sorted_model_performance_df = model_performance_df.sort_values(by=['average score'],ascending=False) \n",
    "    \n",
    "    # find best performing model\n",
    "    best_model_name = sorted_model_performance_df.head(1)['model'].to_string().split(\" \")[4]\n",
    "    print(\"\\n\\n The best Performing model :\", best_model_name)\n",
    "    \n",
    "    # tune hyperparameters of best performing model\n",
    "    tuned_model = tune_hyperparameters(X_train_scaled, y_train, best_model_name)\n",
    "        \n",
    "    # Model performance for test data generated using train test split\n",
    "    \n",
    "    # To print classification report and cohen_kappa_score    \n",
    "    # performance_report = validate_test_groundtruth(tuned_model,X_test_scaled,y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_test_groundtruth(final_model, X_test_scaled, y_test):\n",
    "    \"\"\"\n",
    "    Model Performance validation against test data\n",
    "\n",
    "    Args:\n",
    "        final_model (_type_): _description_\n",
    "        X_test_scaled (_type_): _description_\n",
    "        y_test (_type_): _description_\n",
    "    \"\"\"\n",
    "        \n",
    "    y_predicted = final_model.predict(X_test_scaled)\n",
    "\n",
    "    print(\"\\n\\nFor test data generated using train test split \\n\")\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "\n",
    "\n",
    "    print(f\"Cohen_kappa_score: {cohen_kappa_score(y_test, y_predicted)} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
