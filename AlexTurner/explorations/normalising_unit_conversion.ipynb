{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 71 different unit measurements\n",
    "df_labevents.VALUEUOM.unique()\n",
    "\n",
    "df_labevents[df_labevents['VALUEUOM']== ' ']['ITEMID'].unique()\n",
    "# ITEMID 51498, 51289 (Specific Gravity & Serum Viscosity) have no unit measurement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_fields = ['GENDER']\n",
    "norm_fields= ['AGE','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',\n",
    "    'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',\n",
    "    'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',\n",
    "    'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index',\n",
    "    'PaO2_FiO2','cumulated_balance_tev', 'elixhauser', 'Albumin', 'CO2_mEqL', 'Ionised_Ca']\n",
    "log_fields = ['max_dose_vaso','SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR',\n",
    "              'input_total_tev','input_4hourly_tev','output_total','output_4hourly', 'bloc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name, unit, rng_check_fn, convert_fn\n",
    "UNIT_CONVERSIONS = [\n",
    "    ('weight',                   'oz',  None,             lambda x: x/16.*0.45359237),\n",
    "    ('weight',                   'lbs', None,             lambda x: x*0.45359237),\n",
    "    ('fraction inspired oxygen', None,  lambda x: x > 1,  lambda x: x/100.),\n",
    "    ('oxygen saturation',        None,  lambda x: x <= 1, lambda x: x*100.),\n",
    "    ('temperature',              'f',   lambda x: x > 79, lambda x: (x - 32) * 5./9),\n",
    "    ('height',                   'in',  None,             lambda x: x*2.54),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_by_name_from_df_column_or_index(data_df, colname):\n",
    "    \"\"\" Easily get values for named field, whether a column or an index\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    values : 1D array\n",
    "    \"\"\"\n",
    "    try:\n",
    "        values = data_df[colname]\n",
    "    except KeyError as e:\n",
    "        if colname in data_df.index.names:\n",
    "            values = data_df.index.get_level_values(colname)\n",
    "        else:\n",
    "            raise e\n",
    "    return values\n",
    "\n",
    "def standardise_units(X, name_col='itemid', unit_col='valueuom', value_col='value', inplace=True):\n",
    "    if not inplace: X = X.copy()\n",
    "    name_col_vals = get_values_by_name_from_df_column_or_index(X, name_col)\n",
    "    unit_col_vals = get_values_by_name_from_df_column_or_index(X, unit_col)\n",
    "\n",
    "    try:\n",
    "        name_col_vals = name_col_vals.str\n",
    "        unit_col_vals = unit_col_vals.str\n",
    "    except:\n",
    "        print(\"Can't call *.str\")\n",
    "        print(name_col_vals)\n",
    "        print(unit_col_vals)\n",
    "        raise\n",
    "\n",
    "    name_filter = lambda n: name_col_vals.contains(n, case=False, na=False)\n",
    "    unit_filter = lambda n: unit_col_vals.contains(n, case=False, na=False)\n",
    "\n",
    "    for name, unit, rng_check_fn, convert_fn in UNIT_CONVERSIONS:\n",
    "        name_filter_idx = name_filter(name)\n",
    "        needs_conversion_filter_idx = name_filter_idx & False\n",
    "\n",
    "        if unit is not None: needs_conversion_filter_idx |= name_filter(unit) | unit_filter(unit)\n",
    "        if rng_check_fn is not None: needs_conversion_filter_idx |= rng_check_fn(X[value_col])\n",
    "\n",
    "        idx = name_filter_idx & needs_conversion_filter_idx\n",
    "\n",
    "        X.loc[idx, value_col] = convert_fn(X[value_col][idx])\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise binary fields\n",
    "train_set[binary_fields] = train_set[binary_fields] - 0.5 \n",
    "val_set[binary_fields] = val_set[binary_fields] - 0.5 \n",
    "test_set[binary_fields] = test_set[binary_fields] - 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise normal distribution fields\n",
    "for item in norm_fields:\n",
    "    avg = train_set[item].mean()\n",
    "    std = train_set[item].std()\n",
    "    train_set[item] = (train_set[item] - avg) / std\n",
    "    val_set[item] = (val_set[item] - avg) / std\n",
    "    test_set[item] = (test_set[item] - avg) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise log normal fields\n",
    "train_set[log_fields] = np.log(0.1 + train_set[log_fields])\n",
    "val_set[log_fields] = np.log(0.1 + val_set[log_fields])\n",
    "test_set[log_fields] = np.log(0.1 + test_set[log_fields])\n",
    "for item in log_fields:\n",
    "    avg = train_set[item].mean()\n",
    "    std = train_set[item].std()\n",
    "    train_set[item] = (train_set[item] - avg) / std\n",
    "    val_set[item] = (val_set[item] - avg) / std\n",
    "    test_set[item] = (test_set[item] - avg) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinMax Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale features to [0,1] in train, validation and test sets\n",
    "import copy\n",
    "\n",
    "scalable_fields = copy.deepcopy(binary_fields)\n",
    "scalable_fields.extend(norm_fields)\n",
    "scalable_fields.extend(log_fields)\n",
    "for col in scalable_fields:\n",
    "    minimum = min(train_set[col])\n",
    "    maximum = max(train_set[col])\n",
    "    train_set[col] = (train_set[col] - minimum)/(maximum-minimum)\n",
    "    val_set[col] = (val_set[col] - minimum)/(maximum-minimum)\n",
    "    test_set[col] = (test_set[col] - minimum)/(maximum-minimum)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
