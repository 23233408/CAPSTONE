{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline to automate the model fitting, tuning and evaluation process.\n",
    "\n",
    "#todo:\n",
    "* identify models to trial:\n",
    "  KNN, decision tree, bagging, XGBoost?\n",
    "* test train split: stratified sampling vs. SMOTE\n",
    "* make hyperparameter tuning parameterised for all candidate models\n",
    "  - establish grid paramater space\n",
    "  - set final model parameters...\n",
    "* (AUROCs) for performance evaluation of multiple models\n",
    "* Calibration plots to visualise candidate model predictions\n",
    "* SHAP plot - For best-performing model, identify & report significance of parameters\n",
    "\n",
    "#todo: questions\n",
    "* do we need to scale/normalise?\n",
    "* should we set up n folds for CV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score,RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# model comparison\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier,AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import cohen_kappa_score,classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compare model performance\n",
    "def compare_models(X_train_scaled, y_train, candidate_models, class_weight_dict):\n",
    "\n",
    "    # store model names\n",
    "    model_names = []           \n",
    "    model_average_scores = [] \n",
    "    # store the mean score of n-fold cross validation for each model\n",
    "\n",
    "   \n",
    "   # calculate mean scores using cross validation\n",
    "    for model_name, model in candidate_models.items():\n",
    "        scores = cross_val_score(model, X_train_scaled, y_train)\n",
    "        model_names.append(model_name)\n",
    "        model_average_scores.append(scores.mean())\n",
    "        print(f'Scoring completed for {model_name}')\n",
    "        \n",
    "    # store mean scores for each model\n",
    "    df_model = pd.DataFrame()\n",
    "    df_model['model'] = model_names\n",
    "    df_model['average_score'] = model_average_scores\n",
    "        \n",
    "    print(df_model)\n",
    "    print(\" ---------------------------------------- \")\n",
    "        \n",
    "    return(df_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X_train_scaled, y_train, best_model_name):\n",
    "    \n",
    "    #todo: query the candidate_models var instead of using if else...\n",
    "    \n",
    "    \n",
    "    param_grid = {\n",
    "    'Logistic_Regression': {'C': [0.1, 1, 10],\n",
    "                            'penalty': ['l1', 'l2']\n",
    "                            },\n",
    "    'Random_Forest': {'n_estimators': [50, 100, 150], \n",
    "                      'max_depth': [None, 10, 20], \n",
    "                      'min_leaf': list(range(2, 8)),\n",
    "                      'min_samples_split': list(range(2,25)),\n",
    "                      'min_samples_leaf': list(range(2,25))\n",
    "                      },\n",
    "    'Gradient_Boosting': {'n_estimators': [50, 100, 150, 200], \n",
    "                          'learning_rate': [0.01, 0.1, 0.2, 0.5], \n",
    "                          'loss': ['log_loss', 'exponential'], \n",
    "                          'criterion': ['friedman_mse', 'squared_error'],\n",
    "                          'max_features': ['sqrt', 'log2']\n",
    "                          }\n",
    "    }\n",
    "    \n",
    "    # Create an empty dictionary to store the best parameters for each model\n",
    "    best_params = {}\n",
    "\n",
    "    # Loop through the candidate_models dictionary to perform GridSearchCV for each model\n",
    "    for model_name, model in candidate_models.items():\n",
    "        \n",
    "        #todo:\n",
    "        # gridSearchCV \n",
    "        # or\n",
    "        # RandomizedSearchCV - refit=True\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid[model_name], cv=5)\n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        # Store the best parameters for the current model\n",
    "        best_params[model_name] = grid_search.best_params_\n",
    "        \n",
    "        print(f\"The best parameters for {model_name} are {grid_search.best_params_}\")\n",
    "\n",
    "        # gradient boosing has sample weights, not class weights\n",
    "        # sample weight\n",
    "        \n",
    "        # have to calculate sample/class weight seperately for test and seperately for training\n",
    "        \n",
    "        \n",
    "        gridcv.fit(X_train_scaled, y_train)\n",
    "        print('Optimising complete')\n",
    "        \n",
    "        df_optimiser = pd.DataFrame(gridcv.cv_results_).dropna()\n",
    "        hyper_df = df_optimiser[['param_loss', 'param_learning_rate', 'param_n_estimators', 'param_criterion', 'param_max_features', 'mean_test_score']]\n",
    "        print(hyper_df) \n",
    "        print(\" \\n\\n  ----------------------------------------  \\n\\n\")\n",
    "        \n",
    "        # get best score\n",
    "        best_score = gridcv.best_score_\n",
    "        print(f\"Best score for {best_model_name}: {best_score}\")\n",
    "        \n",
    "        # Init dictionary of best params\n",
    "        best_params = gridcv.best_params_   \n",
    "        \n",
    "        #todo: how to make this configurable? all models will use diff params...\n",
    "        final_model = GradientBoostingClassifier(criterion=best_params['criterion'], \n",
    "                                            learning_rate=best_params['learning_rate'], loss=best_params['loss'], \n",
    "                                            max_features=best_params['max_features'], n_estimators=best_params['n_estimators'])\n",
    "        \n",
    "\n",
    "        hypertuned_model = final_model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # return model object\n",
    "        return(hypertuned_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance validation against test data\n",
    "\n",
    "def validate_test_groundtruth(final_model,X_test_scaled,y_test):\n",
    "    \n",
    "    y_predicted = final_model.predict(X_test_scaled)\n",
    "\n",
    "    print(\"\\n\\nFor test data generated using train test split \\n\")\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "\n",
    "    print(\" \\n\\n  ----------------------------------------  \\n\\n\")\n",
    "\n",
    "    print(f\"cohen_kappa_score: {cohen_kappa_score(y_test,y_predicted)} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_expression(df_processed, test_size):\n",
    "     \"\"\"\n",
    "    Preprocesses the dataframe by encoding the 'Sepsis' column and dropping unnecessary columns.\n",
    "    Splits the data into training and test sets.\n",
    "    Computes class weights for balancing the classes in the target variable.\n",
    "    Normalises numerical features using MinMaxScaler.\n",
    "    Compare performances of different models.\n",
    "    Perform hyperparameter tuning on best model. \n",
    "    Validate the optimal model's performance on the test set.\n",
    "    Generate predictions for unknown data using the optimal model.\n",
    "    \n",
    "    Parameters:\n",
    "    - df_processed:\n",
    "    - test_size: \n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame \n",
    "    \"\"\"\n",
    "        \n",
    "    optimal_model = {}  \n",
    "    class_weight_dict = {} # Dictionary to store class weights\n",
    "    \n",
    "    df_train = df_processed \n",
    "         \n",
    "    # Target and Predictors\n",
    "    X = df_train.drop('IS_SEPSIS', axis='columns')\n",
    "    y = df_train['IS_SEPSIS']\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=test_size, \n",
    "                                                        stratify=y, \n",
    "                                                        random_state=42)\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "    \n",
    "    # Dictionary containning class weights of target class labels\n",
    "    for index, weight in enumerate(class_weights):\n",
    "        class_weight_dict[index] = weight        \n",
    "    \n",
    "    \n",
    "    # Normalise numeric columns, since features have different ranges \n",
    "    # In order to minimise data leakage during model testing the scalar is first fitted to train data and then \n",
    "    # used it to transform the test data      \n",
    "\n",
    "    # Fit the scaler and transform the DataFrame\n",
    "    df_standardised = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "    \n",
    "    # Standardising, not normalising\n",
    "    scaler = StandardScaler()\n",
    "    scaler_fit = scaler.fit(X_train) \n",
    "    X_train_scaled = scaler_fit.transform(X_train)  \n",
    "    X_test_scaled = scaler_fit.transform(X_test)  \n",
    "    \n",
    "\n",
    "    # get model performances\n",
    "    model_performance_df = compare_models(X_train_scaled, y_train, class_weight_dict)  \n",
    "    \n",
    "    # sort by model performance \n",
    "    sorted_model_performance_df = model_performance_df.sort_values(by=['average score'],ascending=False) \n",
    "    \n",
    "    \n",
    "    # find best performing model\n",
    "    best_model_name = sorted_model_performance_df.head(1)['model'].to_string().split(\" \")[4]\n",
    "    print(\"\\n\\n The best Performing model :\", best_model_name)\n",
    "    \n",
    "    # tune hyperparameters\n",
    "    tuned_model = tune_hyperparameters(X_train_scaled, y_train, best_model_name)\n",
    "        \n",
    "    # Model performance for test data generated using train test split\n",
    "    \n",
    "    # To print classification report and cohen_kappa_score    \n",
    "    performance_report = validate_test_groundtruth(optimal_model,X_test_scaled,y_test) \n",
    "    \n",
    "    # Generating the dataframe with predicted values\n",
    "    generate_test_df = validate_test_unknown(optimal_model,scaler_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the parameters\n",
    "test_size = 0.2\n",
    "\n",
    "candidate_models = {\n",
    "    'Logistic_Regression': LogisticRegression(max_iter=10000000000, class_weight=class_weight_dict),\n",
    "    'Random_Forest': RandomForestClassifier(class_weight=class_weight_dict),\n",
    "    'Gradient_Boosting': GradientBoostingClassifier()\n",
    "    \n",
    "    # todo: are these valid?\n",
    "    # 'SGD_Classifier': SGDClassifier(class_weight=class_weight_dict),\n",
    "    # 'XGB': XGBClassifier(),\n",
    "    # 'KNeighbors': KNeighborsClassifier(),\n",
    "    # 'Adaboost':AdaBoostClassifier()\n",
    "    }\n",
    "\n",
    "load_expression(df_processed, test_size, candidate_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
